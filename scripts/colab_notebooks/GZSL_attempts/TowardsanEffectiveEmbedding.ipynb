{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGmtd9TgHsQU"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6el0vsA_a9d-"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from scipy.spatial import KDTree\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import math\n",
        "from itertools import compress, combinations, product\n",
        "from collections import Counter\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from statistics import mean\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "dQhAoshwB4Jl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import fastprogress\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "def printmd(string):\n",
        "    display(Markdown(string))\n",
        "\n",
        "\n",
        "# checks for GPU\n",
        "def get_device(cuda_preference=True):\n",
        "    print('cuda available:', torch.cuda.is_available(), \n",
        "          '; cudnn available:', torch.backends.cudnn.is_available(),\n",
        "          '; num devices:', torch.cuda.device_count())\n",
        "    \n",
        "    use_cuda = False if not cuda_preference else torch.cuda.is_available()\n",
        "    device = torch.device('cuda:0' if use_cuda else 'cpu')\n",
        "    device_name = torch.cuda.get_device_name(device) if use_cuda else 'cpu'\n",
        "    print('Using device', device_name)\n",
        "    return device\n",
        "\n",
        "# trains target domain network\n",
        "def train_target(X_target, y_target, source_emb, optimizer, model, loss_fn,\n",
        "                 device, num_epochs, master_bar):\n",
        "\n",
        "    patience = 50\n",
        "    counter = 0\n",
        "    loss_min = math.inf\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.1, \n",
        "                                                     patience=10, min_lr=0.0001)\n",
        "    for epoch in fastprogress.progress_bar(range(num_epochs), parent=master_bar):\n",
        "        optimizer.zero_grad()\n",
        "        model.train()\n",
        "        target_emb, output = model(X_target.to(device))\n",
        "        loss_total, loss_Reg, loss_CE = loss_fn(target_emb, source_emb, \n",
        "                                                output, y_target.to(device))\n",
        "\n",
        "        if loss_total >= loss_min:\n",
        "          counter += 1\n",
        "        else:\n",
        "          loss_min = loss_total\n",
        "          counter = 0\n",
        "        if counter > patience:\n",
        "          break\n",
        "        scheduler.step(loss_total)\n",
        "        loss_total.backward()\n",
        "        optimizer.step()\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        target_emb, output = model(X_target.to(device))  \n",
        "    return target_emb, loss_total, loss_Reg, loss_CE\n",
        "\n",
        "# trains source domain network\n",
        "def train_source(X_source, target_emb, optimizer, model, loss_fn, num_epochs,\n",
        "                 device, master_bar):\n",
        "    patience = 50\n",
        "    counter = 0\n",
        "    loss_min = math.inf\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor=0.1, \n",
        "                                                     patience=10, min_lr=0.0001)\n",
        "    for epoch in fastprogress.progress_bar(range(num_epochs), parent=master_bar):\n",
        "        optimizer.zero_grad()\n",
        "        model.train()\n",
        "        source_emb = model(X_source.to(device))\n",
        "        loss = loss_fn(target_emb, source_emb)\n",
        "        if loss >= loss_min:\n",
        "          counter += 1\n",
        "        else:\n",
        "          loss_min = loss\n",
        "          counter = 0\n",
        "        if counter > patience:\n",
        "          break\n",
        "        scheduler.step(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        source_emb = model(X_source.to(device))  \n",
        "    return source_emb, loss\n",
        "\n",
        "# predicts class\n",
        "def predict_class(model_target, model_source, X_test, avg_source):\n",
        "    model_source.eval()\n",
        "    with torch.no_grad():\n",
        "        source_emb = model_source(avg_source.to(device))\n",
        "    model_target.eval()\n",
        "    with torch.no_grad():\n",
        "        target_emb, output = model_target(X_test.to(device))    \n",
        "    tree = KDTree(source_emb.cpu().numpy())\n",
        "    distances, predictions = tree.query(target_emb.cpu().numpy(), k=1)\n",
        "    return distances, predictions\n",
        "\n",
        "# runs training\n",
        "def run_training(model_target,  loss_fn_target, model_source, loss_fn_source,\n",
        "                 lr, eta, X_target, X_source, y_target, device, num_iteration,\n",
        "                 num_epochs, verbose=True):\n",
        "    start_time = time.time()\n",
        "    master_bar = fastprogress.master_bar(range(num_iteration))\n",
        "    model_source.eval()\n",
        "    with torch.no_grad():\n",
        "        source_emb = model_source(X_source.to(device))  \n",
        "    for iterarion in master_bar:\n",
        "        optimizer_target = optim.Adam(model_target.parameters(), lr, weight_decay=eta)\n",
        "        optimizer_source = optim.Adam(model_source.parameters(), lr, weight_decay=eta)\n",
        "        target_emb, loss_total, loss_Reg, loss_CE = train_target(X_target, y_target, source_emb, optimizer_target,\n",
        "                                                                 model_target, loss_fn_target, device, num_epochs,\n",
        "                                                                 master_bar)\n",
        "        source_emb, loss = train_source(X_source, target_emb, optimizer_source,\n",
        "                                        model_source, loss_fn_source, num_epochs,\n",
        "                                        device, master_bar)              \n",
        "        if verbose:\n",
        "            master_bar.write(f'target loss: {loss_total.detach().cpu().numpy():.2f}, reg loss: {loss_Reg.detach().cpu().numpy():.2f}, ce loss: {loss_CE.detach().cpu().numpy():.3f}, source loss: {loss.detach().cpu().numpy():.3f}')                \n",
        "    time_elapsed = np.round(time.time() - start_time, 0).astype(int)\n",
        "    print(f'Finished training after {time_elapsed} seconds.')\n",
        "    return\n",
        "\n",
        "\n",
        "# runs semi-supervised\n",
        "def fit_semi_supervised(model_target, loss_fn_target,\n",
        "                        model_source, loss_fn_source, lr, eta,\n",
        "                        X_norm_tens, X_source_avg_tens, X_test_norm_tens, y_seen_tens, avg_source_tens, device, num_iteration,\n",
        "                        num_epochs, missing, K_seen=10, K_unseen=10, part=0.5):    \n",
        "      for k in range(K_unseen):\n",
        "        distances, labels = predict_class(model_target, model_source, X_test_norm_tens, avg_source_tens)\n",
        "        all_idx = np.arange(len(labels))\n",
        "        if sum(np.in1d(labels, missing)) > 1:\n",
        "          idx = np.array([])\n",
        "          for i in missing:\n",
        "            temp = all_idx[np.in1d(labels, i)]\n",
        "            if len(temp) < 2:\n",
        "              continue\n",
        "            lowest_ind = np.argsort(distances[temp])[:math.ceil(len(temp) *((k + 1) / K_unseen) * part)]\n",
        "            idx = np.concatenate((idx, temp[lowest_ind]))\n",
        "          seen = np.in1d(all_idx, idx)\n",
        "          X_target_new = torch.cat((X_norm_tens, X_test_norm_tens[seen]))\n",
        "          y_new = torch.cat((y_seen_tens, torch.LongTensor(labels[seen])))\n",
        "          X_source_new = torch.cat((X_source_avg_tens, avg_source_tens[labels[seen]]))\n",
        "          print(sum(seen))\n",
        "          run_training(model_target, loss_fn_target,\n",
        "                      model_source, loss_fn_source, lr, eta,\n",
        "                      X_target_new, X_source_new, y_new, device, num_iteration,\n",
        "                      num_epochs, verbose=True)\n",
        "\n",
        "      for k in range(K_seen):\n",
        "        distances, labels = predict_class(model_target, model_source, X_test_norm_tens, avg_source_tens)\n",
        "        all_idx = np.arange(len(labels))\n",
        "        lowest_ind = np.argsort(distances)[:math.ceil(len(distances) *((k + 1)/ K_seen))]\n",
        "        idx = all_idx[lowest_ind]\n",
        "        seen = np.in1d(all_idx, idx)\n",
        "        print(sum(seen))\n",
        "        X_target_new = torch.cat((X_norm_tens, X_test_norm_tens[seen]))\n",
        "        y_new = torch.cat((y_seen_tens, torch.LongTensor(labels[seen])))\n",
        "        X_source_new = torch.cat((X_source_avg_tens, avg_source_tens[labels[seen]]))\n",
        "        run_training(model_target, loss_fn_target,\n",
        "                     model_source, loss_fn_source, lr, eta,\n",
        "                     X_target_new, X_source_new, y_new, device, num_iteration,\n",
        "                     num_epochs, verbose=True)\n",
        "\n",
        "      return\n",
        "\n",
        "# Specific loss for target network\n",
        "class CombLoss(nn.Module):\n",
        "    def __init__(self, lam):\n",
        "        super(CombLoss, self).__init__()\n",
        "        self.lam = lam\n",
        " \n",
        "    def forward(self, target_emb, source_emb, y_pred, y_true):\n",
        "\n",
        "      loss_Reg = nn.MSELoss(reduction=\"sum\")(target_emb, source_emb)\n",
        "      loss_CE = nn.CrossEntropyLoss(reduction=\"sum\")(y_pred, y_true)\n",
        "      loss_total = loss_Reg + self.lam * loss_CE\n",
        "\n",
        "      return(loss_total, loss_Reg, loss_CE)\n",
        "\n",
        "# Builds visual (target) network\n",
        "class TAEM_target(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out, n_classes):\n",
        "        super(TAEM_target, self).__init__()\n",
        "        self.linear1 = nn.Linear(dim_in, dim_out)\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(dim_out, n_classes)\n",
        "    \n",
        "    def forward(self, x):      \n",
        "        x = self.linear1(x)\n",
        "        x = self.act1(x)\n",
        "        y = self.linear2(x)\n",
        "        return x, y\n",
        "\n",
        "\n",
        "# Builds sematic (source) network\n",
        "class TAEM_source(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out, n_hidden):\n",
        "        super(TAEM_source, self).__init__()\n",
        "        self.linear1 = nn.Linear(dim_in, n_hidden)\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(n_hidden, dim_out)\n",
        "        self.act2 = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x):       \n",
        "        x = self.linear1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.act2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# creates source means for each class\n",
        "def get_class_attributes(X_source, y_source, y_target):\n",
        "    avg_source = X_source.groupby(y_source).mean()\n",
        "    avg_source = MinMaxScaler().fit_transform(avg_source)\n",
        "    X_source_avg = np.zeros((len(y_target), 50))\n",
        "    for i in range(len(y_target)):\n",
        "        X_source_avg[i] = avg_source[y_target[i]]\n",
        "    return X_source_avg, avg_source\n",
        "\n",
        "\n",
        "# balancing seen classes\n",
        "def balance_sampling(X, y, n=100):\n",
        "\n",
        "    warnings.filterwarnings('ignore')\n",
        "    counts = Counter(y)\n",
        "    under = np.array([], dtype=\"int32\")\n",
        "    over = np.array([], dtype=\"int32\")\n",
        "    for i in counts.keys():\n",
        "        if counts[i] <= n:\n",
        "            over = np.concatenate((over, np.array([i])))\n",
        "        else:\n",
        "            under = np.concatenate((under, np.array([i])))\n",
        "    if len(over) == 0:\n",
        "        dict_under = dict(zip(under, [n for i in range(len(under))]))\n",
        "        under_sam =  RandomUnderSampler(sampling_strategy=dict_under)\n",
        "        X_under, y_under = under_sam.fit_resample(X, y)\n",
        "        return X_under, y_under\n",
        "    elif len(under) == 0:\n",
        "        dict_over = dict(zip(over, [n for i in range(len(over))]))\n",
        "        over_sam = SMOTE(sampling_strategy=dict_over)\n",
        "        X_over, y_over = over_sam.fit_resample(X, y)\n",
        "        return X_over, y_over\n",
        "    else:\n",
        "        if len(over) == 1:\n",
        "            # Tricks SMOTE into oversampling one class\n",
        "            pseudo_X = np.full((n, X.shape[1]), 10000)\n",
        "            pseudo_y = np.full(n, 10000)\n",
        "            dict_over = dict()\n",
        "            dict_over[over[0]] = n\n",
        "            dict_over[10000] = n\n",
        "            is_over = np.in1d(y, over)\n",
        "            over_sam = SMOTE(sampling_strategy=dict_over)\n",
        "            is_over = np.in1d(y, over)\n",
        "            X_over_, y_over_ = over_sam.fit_resample(np.concatenate((X[is_over], pseudo_X)),\n",
        "                                                     np.concatenate((y[is_over], pseudo_y)))\n",
        "            X_over = X_over_[y_over_==over[0]]\n",
        "            y_over = y_over_[y_over_==over[0]]\n",
        "\n",
        "        else:\n",
        "            dict_over = dict(zip(over, [n for i in range(len(over))]))\n",
        "            over_sam = SMOTE(sampling_strategy=dict_over)\n",
        "            is_over = np.in1d(y, over)\n",
        "            X_over, y_over = over_sam.fit_resample(X[is_over], y[is_over])\n",
        "\n",
        "        if len(under) == 1:\n",
        "            # Tricks RandomUnderSampler into working with one class\n",
        "            pseudo_X = np.full((n, X.shape[1]), 10000)\n",
        "            pseudo_y = np.full(n, 10000)\n",
        "            dict_under = dict()\n",
        "            dict_under[under[0]] = n\n",
        "            dict_under[10000] = n\n",
        "            is_under = np.in1d(y, under)\n",
        "            under_sam = RandomUnderSampler(sampling_strategy=dict_under)\n",
        "            is_under = np.in1d(y, under)\n",
        "            X_under_, y_under_ = under_sam.fit_resample(np.concatenate((X[is_under], pseudo_X)),\n",
        "                                                        np.concatenate((y[is_under], pseudo_y)))\n",
        "            X_under = X_under_[y_under_==under[0]]\n",
        "            y_under = y_under_[y_under_==under[0]]\n",
        "        else:\n",
        "            dict_under = dict(zip(under, [n for i in range(len(under))]))\n",
        "            under_sam = RandomUnderSampler(sampling_strategy=dict_under)\n",
        "            is_under = np.in1d(y, under)\n",
        "            X_under, y_under = under_sam.fit_resample(X[is_under], y[is_under])\n",
        "\n",
        "        X_combined_sampling = np.concatenate((X_over, X_under))\n",
        "        y_combined_sampling = np.concatenate((y_over, y_under))\n",
        "        return X_combined_sampling, y_combined_sampling\n",
        "\n",
        "\n",
        "def split_masked_cells(X_t, y_t, masked_cells, balance=False, n=500):\n",
        "    \"\"\"\n",
        "    Maskes cells for generalized zero-shot learning\n",
        "    :param X_t: feature matrix of target data\n",
        "    :param y_t: labels of target data\n",
        "    :param masked_cells: list of cells to be masked from data\n",
        "    :param balance: whether to balance seen train data\n",
        "    :param n: desired number of samples per class\n",
        "    :return: features of seen classes, features of unseen classes, labels seen classes, labels unseen classes\n",
        "    \"\"\"\n",
        "    keep = np.in1d(y_t, masked_cells, invert=True)\n",
        "    X_t_seen = X_t[keep]\n",
        "    X_t_unseen = X_t[~keep]\n",
        "    y_seen = y_t[keep]\n",
        "    y_unseen = y_t[~keep]\n",
        "    if balance:\n",
        "        X_t_seen, y_seen = balance_sampling(X_t_seen, y_seen, n)\n",
        "    return X_t_seen, X_t_unseen, y_seen, y_unseen\n",
        "\n",
        "\n",
        "def h_score(y_true, y_pred, masked_cells):\n",
        "    \"\"\"\n",
        "    H score\n",
        "    :param y_true: true values\n",
        "    :param y_pred: predictions\n",
        "    :param masked_cells: list of masked cells\n",
        "    :return: h-score, acc. of known classes, acc of unknown classes\n",
        "    \"\"\"\n",
        "    warnings.filterwarnings('ignore')\n",
        "    known = np.in1d(y_true, masked_cells, invert=True)\n",
        "    acc_known = balanced_accuracy_score(y_true[known], y_pred[known])\n",
        "    acc_unknown = balanced_accuracy_score(y_true[~known], y_pred[~known])\n",
        "    h = (2 * acc_known * acc_unknown) / (acc_known + acc_unknown)\n",
        "    return h, acc_known, acc_unknown\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter selection and run of one masked combination, takes very, very long!!!\n",
        "def run_comb(X_source, y_source, X_train, y_train, X_test, y_test, combination):\n",
        "      \n",
        "      X_seen, X_unseen, y_seen, y_unseen = split_masked_cells(X_train, y_train, combination)\n",
        "      X_source_avg, avg_source = get_class_attributes(X_source, y_source, y_seen)\n",
        "      scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "      X_norm = scaler.fit_transform(X_seen)\n",
        "      X_norm_tens = torch.FloatTensor(X_norm)\n",
        "      y_seen_tens = torch.LongTensor(y_seen)\n",
        "      X_test_norm = scaler.transform(X_test)\n",
        "      X_test_norm_tens = torch.FloatTensor(X_test_norm)\n",
        "      avg_source_tens = torch.FloatTensor(avg_source)\n",
        "      X_source_avg_tens = torch.FloatTensor(X_source_avg)\n",
        "      eta = [0.001, 0.01]\n",
        "      lam = [0.2, 0.35, 0.5]\n",
        "      lr =.001\n",
        "      num_epochs = 5000\n",
        "      num_iteration = 5\n",
        "      n_common = 50\n",
        "      current_best = 0\n",
        "      params = dict()\n",
        "      for e in eta:\n",
        "        for l in lam:\n",
        "            score = []\n",
        "            print(\"Testing eta=\"+str(e)+\", lambda=\"+str(l))\n",
        "            for train_index, test_index in KFold(shuffle=True, n_splits=5).split(X_seen):\n",
        "                X_train_val, X_val = X_norm_tens[train_index], X_norm_tens[test_index]\n",
        "                y_train_val, y_val = y_seen_tens[train_index], y_seen_tens[test_index]\n",
        "                X_source_avg_tens_val = X_source_avg_tens[train_index]\n",
        "                for i in set(y_seen):\n",
        "                   X_seen_val, X_unseen_val, y_seen_val, y_unseen_val = split_masked_cells(X_train_val, y_train_val, masked_cells=[i])\n",
        "                   X_source_avg_seen, _res, _res2, _res3 = split_masked_cells(X_source_avg_tens_val, y_train_val, masked_cells=[i])                \n",
        "                   loss_fn_target = CombLoss(lam=l)\n",
        "                   loss_fn_source = nn.MSELoss(reduction=\"sum\")\n",
        "                   model_target = TAEM_target(50, n_common, 11)\n",
        "                   model_target.to(device)\n",
        "                   model_source = TAEM_source(50, n_common, 32)\n",
        "                   model_source.to(device)\n",
        "\n",
        "                   run_training(model_target, loss_fn_target,\n",
        "                                model_source, loss_fn_source, lr, e,\n",
        "                                X_seen_val, X_source_avg_seen, y_seen_val, device, num_iteration,\n",
        "                                num_epochs)\n",
        "            \n",
        "                   distances, labels = predict_class(model_target, model_source, X_val, avg_source_tens)\n",
        "                   h, acc_known, acc_unknown = h_score(y_val.numpy(), labels, [i])\n",
        "                   score.append(h)\n",
        "            score = mean(score)\n",
        "            if score > current_best:\n",
        "              current_best = score\n",
        "              params[\"eta\"] = e\n",
        "              params[\"lam\"] = l\n",
        "      \n",
        "      print(params)\n",
        "      loss_fn_target = CombLoss(lam=params[\"lam\"])\n",
        "      loss_fn_source = nn.MSELoss(reduction=\"sum\")\n",
        "      model_target = TAEM_target(50, n_common, 11)\n",
        "      model_target.to(device)\n",
        "      model_source = TAEM_source(50, n_common, 32)\n",
        "      model_source.to(device)      \n",
        "      run_training(model_target, loss_fn_target,\n",
        "                         model_source, loss_fn_source, lr, params[\"eta\"],\n",
        "                         X_norm_tens, X_source_avg_tens, y_seen_tens, device, num_iteration,\n",
        "                         num_epochs, verbose=True)    \n",
        "\n",
        "      distances, labels = predict_class(model_target, model_source, X_test_norm_tens, avg_source_tens)   \n",
        "      h, acc_known, acc_unknown = h_score(y_test, labels, combination)   \n",
        "\n",
        "\n",
        "      fit_semi_supervised(model_target, loss_fn_target,\n",
        "                          model_source, loss_fn_source, lr, params[\"eta\"],\n",
        "                          X_norm_tens, X_source_avg_tens, X_test_norm_tens, y_seen_tens, avg_source_tens, device, num_iteration,\n",
        "                          num_epochs, combination, K_seen=10, K_unseen=10, part=0.5)         \n",
        "            \n",
        "      distances, labels = predict_class(model_target, model_source, X_test_norm_tens, avg_source_tens)\n",
        "      h_ssl, acc_known_ssl, acc_unknown_ssl = h_score(y_test, labels, combination)\n",
        "      return labels, h, acc_known, acc_unknown, h_ssl, acc_known_ssl, acc_unknown_ssl"
      ],
      "metadata": {
        "id": "LjZv1wLNMvh7"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQWeCNS-H0wZ"
      },
      "source": [
        "# Preparations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqQNwqLi-Gpb",
        "outputId": "eae3802a-ca9f-4925-cfee-e26dd2c6254c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmsWxrK8gJ6r",
        "outputId": "9504d8a8-ca18-4510-a5fc-ec7f41b1effe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda available: True ; cudnn available: True ; num devices: 1\n",
            "Using device Tesla T4\n"
          ]
        }
      ],
      "source": [
        "device = get_device()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emCIuFMOgHe3"
      },
      "source": [
        "Play around..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWceHVc_-YeD"
      },
      "outputs": [],
      "source": [
        "X_source = pd.read_csv(\"/content/drive/MyDrive/data/brain_mouse_red_scetm.csv\", index_col=0)\n",
        "y_source = pd.read_csv(\"/content/drive/MyDrive/data/brain_mouse_red_label.csv\", index_col=0)[\"label\"]\n",
        "X_train = pd.read_csv(\"/content/drive/MyDrive/data/brain_human_red_train_scetm.csv\", index_col=0)\n",
        "y_train = pd.read_csv(\"/content/drive/MyDrive/data/brain_human_red_train_label.csv\", index_col=0)[\"label\"]\n",
        "X_test = pd.read_csv(\"/content/drive/MyDrive/data/brain_human_red_test_scetm.csv\", index_col=0)\n",
        "y_test = pd.read_csv(\"/content/drive/MyDrive/data/brain_human_red_test_label.csv\", index_col=0)[\"label\"]\n",
        "X_train, y_train = balance_sampling(X_train, y_train, 300)\n",
        "combination = [6, 9]\n",
        "result = run_comb(X_source, y_source, X_train, y_train, X_test, y_test, combination)\n",
        "results"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}