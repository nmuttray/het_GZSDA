{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "oGmtd9TgHsQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import normalize, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import math\n",
        "from itertools import compress, combinations, product\n",
        "from collections import Counter\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from statistics import mean\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
      ],
      "metadata": {
        "id": "6el0vsA_a9d-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQhAoshwB4Jl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import fastprogress\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "\n",
        "# gets device\n",
        "from IPython.display import Markdown, display\n",
        "def printmd(string):\n",
        "    display(Markdown(string))\n",
        "\n",
        "def get_device(cuda_preference=True):\n",
        "    print('cuda available:', torch.cuda.is_available(), \n",
        "          '; cudnn available:', torch.backends.cudnn.is_available(),\n",
        "          '; num devices:', torch.cuda.device_count())\n",
        "    \n",
        "    use_cuda = False if not cuda_preference else torch.cuda.is_available()\n",
        "    device = torch.device('cuda:0' if use_cuda else 'cpu')\n",
        "    device_name = torch.cuda.get_device_name(device) if use_cuda else 'cpu'\n",
        "    print('Using device', device_name)\n",
        "    return device\n",
        "\n",
        "\n",
        "# trains network\n",
        "def train(train_dataloader, optimizer, model, loss_fn, \n",
        "                 device, master_bar):\n",
        "\n",
        "    epoch_loss=[]\n",
        "    for X, y in fastprogress.progress_bar(train_dataloader, parent=master_bar):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        model.train()\n",
        "        \n",
        "        # Forward\n",
        "        y_pred = model(X.to(device))\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(y_pred.to(device), y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss.append(loss.item())\n",
        "    return np.mean(epoch_loss)\n",
        "\n",
        "\n",
        "# predicts class\n",
        "def predict_class(model, X_test):   \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      output = model(X_test.to(device))\n",
        "      pred = torch.argmax(output, 1)\n",
        "    return pred, output\n",
        "\n",
        "# trainings wrapper\n",
        "def run_training(model, loss_fn, lr, eta, train_dataloader, device, num_epochs):\n",
        "    \"\"\" Run model training \"\"\"\n",
        "    optimizer = optim.Adam(model.parameters(), lr, weight_decay=eta)\n",
        "\n",
        "    start_time = time.time()\n",
        "    master_bar = fastprogress.master_bar(range(num_epochs))\n",
        "    train_losses = []\n",
        "    for epoch in master_bar:       \n",
        "        epoch_train_loss = train(train_dataloader, optimizer, model, loss_fn, device, master_bar)\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        master_bar.write(f'Train loss: {epoch_train_loss:.3f}')      \n",
        "    time_elapsed = np.round(time.time() - start_time, 0).astype(int)\n",
        "    print(f'Finished training after {time_elapsed} seconds.')\n",
        "    return\n",
        "\n",
        "# QFSL model\n",
        "class QFSL(nn.Module):    \n",
        "    \n",
        "    def __init__(self, dim_in, dim_out, n_classes, dim_hidden = 50, hidden=True):\n",
        "        super(QFSL, self).__init__()\n",
        "        self.model = []\n",
        "        if hidden:\n",
        "          self.model.append(nn.Linear(dim_in, dim_hidden))\n",
        "          self.model.append(nn.ReLU())\n",
        "          self.model.append(nn.Linear(dim_hidden, dim_out))\n",
        "        else:\n",
        "          self.model.append(nn.Linear(dim_in, dim_out))\n",
        "        self.model.append(nn.ReLU())\n",
        "        self.model.append(nn.Linear(dim_out, n_classes))\n",
        "        self.model = nn.ModuleList(self.model)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        for layer in self.model:\n",
        "          x = layer(x)\n",
        "        return x\n",
        "    # set weights according to mean source information\n",
        "    def set_weights(self, avg_source):\n",
        "      self.model[-1].weight = nn.Parameter(torch.FloatTensor(avg_source), False)\n",
        "      return\n",
        "\n",
        "# creates class attributes from source\n",
        "def get_class_attributes(X_source, y_source):\n",
        "  avg_source = normalize(X_source.groupby(y_source).mean().to_numpy())\n",
        "  return avg_source\n",
        "\n",
        "\n",
        "# rebalances data\n",
        "def balance_sampling(X, y, n=100):\n",
        "    \"\"\"\n",
        "    Re-balances data by over-sampling with SMOTE and under-sampling randomly\n",
        "    :param X: feature matrix\n",
        "    :param y: labels\n",
        "    :param n: desired samples per class\n",
        "    :return: resampled feature matrix, resampled labels\n",
        "    \"\"\"\n",
        "    warnings.filterwarnings('ignore')\n",
        "    counts = Counter(y)\n",
        "    under = np.array([], dtype=\"int32\")\n",
        "    over = np.array([], dtype=\"int32\")\n",
        "    for i in counts.keys():\n",
        "        if counts[i] <= n:\n",
        "            over = np.concatenate((over, np.array([i])))\n",
        "        else:\n",
        "            under = np.concatenate((under, np.array([i])))\n",
        "    if len(over) == 0:\n",
        "        dict_under = dict(zip(under, [n for i in range(len(under))]))\n",
        "        under_sam =  RandomUnderSampler(sampling_strategy=dict_under)\n",
        "        X_under, y_under = under_sam.fit_resample(X, y)\n",
        "        return X_under, y_under\n",
        "    elif len(under) == 0:\n",
        "        dict_over = dict(zip(over, [n for i in range(len(over))]))\n",
        "        over_sam = SMOTE(sampling_strategy=dict_over)\n",
        "        X_over, y_over = over_sam.fit_resample(X, y)\n",
        "        return X_over, y_over\n",
        "    else:\n",
        "        if len(over) == 1:\n",
        "            # Tricks SMOTE into oversampling one class\n",
        "            pseudo_X = np.full((n, X.shape[1]), 10000)\n",
        "            pseudo_y = np.full(n, 10000)\n",
        "            dict_over = dict()\n",
        "            dict_over[over[0]] = n\n",
        "            dict_over[10000] = n\n",
        "            is_over = np.in1d(y, over)\n",
        "            over_sam = SMOTE(sampling_strategy=dict_over)\n",
        "            is_over = np.in1d(y, over)\n",
        "            X_over_, y_over_ = over_sam.fit_resample(np.concatenate((X[is_over], pseudo_X)),\n",
        "                                                     np.concatenate((y[is_over], pseudo_y)))\n",
        "            X_over = X_over_[y_over_==over[0]]\n",
        "            y_over = y_over_[y_over_==over[0]]\n",
        "\n",
        "        else:\n",
        "            dict_over = dict(zip(over, [n for i in range(len(over))]))\n",
        "            over_sam = SMOTE(sampling_strategy=dict_over)\n",
        "            is_over = np.in1d(y, over)\n",
        "            X_over, y_over = over_sam.fit_resample(X[is_over], y[is_over])\n",
        "\n",
        "        if len(under) == 1:\n",
        "            # Tricks RandomUnderSampler into working with one class\n",
        "            pseudo_X = np.full((n, X.shape[1]), 10000)\n",
        "            pseudo_y = np.full(n, 10000)\n",
        "            dict_under = dict()\n",
        "            dict_under[under[0]] = n\n",
        "            dict_under[10000] = n\n",
        "            is_under = np.in1d(y, under)\n",
        "            under_sam = RandomUnderSampler(sampling_strategy=dict_under)\n",
        "            is_under = np.in1d(y, under)\n",
        "            X_under_, y_under_ = under_sam.fit_resample(np.concatenate((X[is_under], pseudo_X)),\n",
        "                                                        np.concatenate((y[is_under], pseudo_y)))\n",
        "            X_under = X_under_[y_under_==under[0]]\n",
        "            y_under = y_under_[y_under_==under[0]]\n",
        "        else:\n",
        "            dict_under = dict(zip(under, [n for i in range(len(under))]))\n",
        "            under_sam = RandomUnderSampler(sampling_strategy=dict_under)\n",
        "            is_under = np.in1d(y, under)\n",
        "            X_under, y_under = under_sam.fit_resample(X[is_under], y[is_under])\n",
        "\n",
        "        X_combined_sampling = np.concatenate((X_over, X_under))\n",
        "        y_combined_sampling = np.concatenate((y_over, y_under))\n",
        "        return X_combined_sampling, y_combined_sampling\n",
        "\n",
        "# splits unknown cells\n",
        "def split_masked_cells(X_t, y_t, masked_cells, balance=False, n=500):\n",
        "    \"\"\"\n",
        "    Maskes cells for generalized zero-shot learning\n",
        "    :param X_t: feature matrix of target data\n",
        "    :param y_t: labels of target data\n",
        "    :param masked_cells: list of cells to be masked from data\n",
        "    :param balance: whether to balance seen train data\n",
        "    :param n: desired number of samples per class\n",
        "    :return: features of seen classes, features of unseen classes, labels seen classes, labels unseen classes\n",
        "    \"\"\"\n",
        "    keep = np.in1d(y_t, masked_cells, invert=True)\n",
        "    X_t_seen = X_t[keep]\n",
        "    X_t_unseen = X_t[~keep]\n",
        "    y_seen = y_t[keep]\n",
        "    y_unseen = y_t[~keep]\n",
        "    if balance:\n",
        "        X_t_seen, y_seen = balance_sampling(X_t_seen, y_seen, n)\n",
        "    return X_t_seen, X_t_unseen, y_seen, y_unseen\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparations"
      ],
      "metadata": {
        "id": "iQWeCNS-H0wZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqQNwqLi-Gpb",
        "outputId": "4e8788de-edb3-4b96-9a68-cb9129b2ffdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = get_device()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmsWxrK8gJ6r",
        "outputId": "3841350e-3888-4089-b706-c00760bc1b12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda available: True ; cudnn available: True ; num devices: 1\n",
            "Using device Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_source = pd.read_csv(\"/content/drive/MyDrive/data/lung_mouse_red_scetm.csv\", index_col=0)\n",
        "y_source = pd.read_csv(\"/content/drive/MyDrive/data/lung_mouse_red_label.csv\", index_col=0)[\"label\"]\n",
        "X_avg = get_class_attributes(X_source, y_source)\n",
        "X_train = pd.read_csv(\"/content/drive/MyDrive/data/lung_human_red_train_scetm.csv\", index_col=0)\n",
        "y_train = pd.read_csv(\"/content/drive/MyDrive/data/lung_human_red_train_label.csv\", index_col=0)[\"label\"]\n",
        "X_test = pd.read_csv(\"/content/drive/MyDrive/data/lung_human_red_test_scetm.csv\", index_col=0)\n",
        "y_test = pd.read_csv(\"/content/drive/MyDrive/data/lung_human_red_test_label.csv\", index_col=0)[\"label\"].to_numpy()\n",
        "X_train, y_train = balance_sampling(X_train, y_train, 300)"
      ],
      "metadata": {
        "id": "-ljwb1USQr2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comb = [1, 2]\n",
        "X_seen, X_unseen, y_seen, y_unseen = split_masked_cells(X_train, y_train, masked_cells=comb)\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "X_norm = scaler.fit_transform(X_seen)\n",
        "X_norm_tens = torch.FloatTensor(X_norm)\n",
        "y_seen_tens = torch.LongTensor(y_seen)\n",
        "X_test_norm = scaler.transform(X_test)\n",
        "X_test_norm_tens = torch.FloatTensor(X_test_norm)\n",
        "train_data = []\n",
        "for i in range(len(y_seen_tens)):\n",
        "   train_data.append([X_norm_tens[i], y_seen_tens[i]])\n",
        "batch_size = 64\n",
        "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True,drop_last=False)"
      ],
      "metadata": {
        "id": "vAdJxC-v9MMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Play around..."
      ],
      "metadata": {
        "id": "jpBxF5xvPbMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr =.0001\n",
        "num_epochs = 1000\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "model = QFSL(50, 50, 11)\n",
        "model.set_weights(X_avg)\n",
        "model.to(device)\n",
        "\n",
        "run_training(model, loss_fn, lr, lam, train_dataloader, device, num_epochs)\n",
        "pred, output = predict_class(model, X_test_norm_tens)\n",
        "pred = pred.detach().cpu().numpy()\n",
        "conf = confusion_matrix(y_test, pred, labels = range(11), normalize = 'true')\n",
        "ConfusionMatrixDisplay(conf, display_labels = range(11)).plot()"
      ],
      "metadata": {
        "id": "apyQ_eNzHxio"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}